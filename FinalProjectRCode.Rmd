---
title: "Appendix For Final Project"
author: "Proud Jiao, Xiaocong Xuan, Yuetong Li"
output: pdf_document
---
## Question 1. Propose a fractional factorial design for the problem. In addition, propose an experimental design constructed using the optimal design approach.

We have 7 parameters to investigate. However, a complete replicate of such design would require $2^7=128$ runs, which outgrows our resources. Due to budgetary constraints, we can try creating a $2^{7-2}$ fractional design with 32 runs. The fractional design generated by FrF2 library is demonstrated below:

```{r message=FALSE, warning=FALSE}
library(FrF2)
# Fractional factorial design
factors <- list(ntree = c(100, 1000),
                mtry = c(2, 6),
                replace = c(0, 1),
                nodesize = c(1,11),
                classwt = c(0.5, 0.9),
                cutoff = c(0.2, 0.8),
                maxnodes = c(10, 1000))
factorial_design <- FrF2(nruns = 32, 
                  nfactors = 7, 
                  alias.info = 2, 
                  randomize = F, 
                  factor.names = factors)
summary(factorial_design)
```

Alternatively, we could have generated a D-optimal design. The advantage of a optimal design is that is more flexible in the number of runs. It saves the cost  by ignoring the less important higher-order interactions. Therefore, if we are only interested in main effects and two-factor interactions, then we only need to estimate:

* 1 intercept
* 7 main effects
* 7 Ã— 6 / 2 = 21 two-factor interaction effects

An experiment with 1 + 7 + 21 = 29 observations can do the job!

However, since we have spare runs up to 35 runs. We will use a 32-run D-optimal design, which will make stronger inferences than a 29-run optimal design. We will leave 3 runs for confirmation experiments. The 32-run optimal design in coded form is as follows and also compare them:

```{r}
# Constructing a D-optimal design
library(AlgDesign)
candidate.set <- gen.factorial(levels=2, 
                               nVars = 7, 
                               varNames = names(factors))
optimal_design <- optFederov(~(ntree + mtry + replace + nodesize + classwt + cutoff + maxnodes)^2, candidate.set, nTrials = 32, nRepeats = 250)
optimal_design$design
```

```{r}
# Constructing an I-optimal design
optimal_design_I <- optFederov(~(ntree + mtry + replace + nodesize + classwt + cutoff + maxnodes)^2, candidate.set, nTrials = 32, criterion = "I", nRepeats = 250)
print.data.frame(optimal_design_I$design)
I.opt <- optimal_design_I$design

# We compare the D-optimal design and I-optimal design using the FDS plot.
library(Vdgraph)
D.opt <- optimal_design$design
Compare2FDS(I.opt, D.opt, "I-optimal", "D-optimal", mod = 1)
```


## Question 2. Compare the optimal design with the fractional factorial design in practical and statistical terms. For instance, what is the performance of the designs for studying the main effects of the tuning parameters only? Can they estimate all two-parameter interactions? Why or why not? How do they compare in terms of multicollinearity?

To measure the performance of a design, we check on aliasing and collinearity between the effects of investigation, in this case, main effects and interaction terms.

```{r warning=FALSE}
library(corrplot)
# Fractional design:
# Create the model matrix including main effects and two-factor interactions.
X.one <- model.matrix(~(ntree + replace + mtry + nodesize + maxnodes + classwt + cutoff)^2-1, data.frame(desnum(factorial_design)))

# Create color map on pairwise correlations.
contrast.vectors.correlations.one <- cor(X.one)
corrplot(contrast.vectors.correlations.one, type = "full", addgrid.col = "gray", tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.5)
```

For the fractional design, no alias exists between main effects. This is good for the estimation of significance of the main effects. However, alias does exist between two-factor interactions. We won't be able to estimate those two-factor interactions with alias.

```{r}
# I-optimal design:
# Create the model matrix including main effects and two-factor interactions.
X.opt <- model.matrix(~(ntree + mtry + replace + nodesize + classwt + cutoff + maxnodes)^2-1, data.frame(I.opt))
# Create color map on pairwise correlations.
contrast.vectors.correlations.opt <- cor(X.opt)

X.opt <- model.matrix(~(ntree + mtry + replace + nodesize + classwt + cutoff + maxnodes)^2, data.frame(I.opt))
corrplot(contrast.vectors.correlations.opt, type = "full", addgrid.col = "gray", tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.5)
```

The color map shows that there is some partial aliasing in the design. We can further inspect the estimation properties of the I-optimal design using the VIFs.

```{r}
XtX <- t(X.opt)%*%X.opt
inv.XtX <- solve(XtX)
var.eff <- diag(inv.XtX)
cat("\n Variance inflation factors \n")
print(nrow(I.opt)*var.eff)
```


## Question 4. Using a commercial software, the TAs and I came up with the experimental design shown in Table 2. How does your recommended design in the previous question compare with this one?

```{r}
# Alternative 22-run design produced by a commercial software (Table 2)
# Transform to the coded level
ntree <- c(100, 550, 1000, 1000, 1000, 100, 1000, 100, 100, 100,
           100, 1000, 100, 550, 100, 1000, 1000, 1000, 100, 1000,
           550, 550)
c_ntree <- c(-1, 0, 1, 1, 1, -1, 1, -1, -1, -1, 
             -1, 1, -1, 0, -1, 1, 1, 1, -1, 1,
             0, 0)

replace <- c(1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
             0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
             1, 1)
c_replace <- c(1, -1, 1, 1, -1, -1, -1, -1, 1, -1,
             -1, 1, 1, -1, -1, -1, -1, 1, 1, 1,
             1, 1)

mtry <- c(2, 2, 4, 6, 6, 2, 2, 2, 6, 6,
          4, 2, 6, 4, 6, 6, 2, 6, 2, 2,
          4, 6)
c_mtry <- c(-1, -1, 0, 1, 1, -1, -1, -1, 1, 1,
          0, -1, 1, 0, 1, 1, -1, 1, -1, -1,
          0, 1)

nodesize <- c(11, 1, 1, 1, 1, 1, 6, 11, 1, 1,
              11, 11, 6, 6, 11, 11, 11, 11, 1, 1,
              6, 11)
c_nodesize <- c(1, -1, -1, -1, -1, -1, 0, 1, -1, -1,
              1, 1, 0, 0, 1, 1, 1, 1, -1, -1,
              0, 1)


maxnodes <- c(10, 10, 10, 1000, 1000, 1000, 10, 10, 10, 10,
              1000, 1000, 1000, 505, 505, 10, 1000, 10, 1000, 505,
              505, 1000)
c_maxnodes <- c(-1, -1, -1, 1, 1, 1, -1, -1, -1, -1,
              1, 1, 1, 0, 0, -1, 1, -1, 1, 0,
              0, 1)

classwt <- c(0.5, 0.5, 0.5, 0.5, 0.9, 0.5, 0.9, 0.9, 0.7, 0.9,
             0.9, 0.5, 0.5, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.9,
             0.7, 0.9)
c_classwt <- c(-1, -1, -1, -1, 1, -1, 1, 1, 0, 1,
             1, -1, -1, 0, -1, -1, 0, 1, 1, 1,
             0, 1)

cutoff <- c(0.8, 0.2, 0.2, 0.8, 0.2, 0.8, 0.8, 0.2, 0.8, 0.5,
            0.8, 0.5, 0.2, 0.5, 0.2, 0.8, 0.2, 0.2, 0.2, 0.8,
            0.5, 0.8)
c_cutoff <- c(1, -1, -1, 1, -1, 1, 1, -1, 1, 0,
            1, 0, -1, 0, -1, 1, -1, -1, -1, 1,
            0, 1)

alter <- cbind(ntree, mtry, replace, nodesize, classwt, cutoff, maxnodes)
alter <- data.frame(alter)

c_alter <- cbind(c_ntree, c_mtry, c_replace, c_nodesize, 
                 c_classwt, c_cutoff, c_maxnodes)
c_alter <- data.frame(c_alter)

alter; c_alter
```

The alternatively designed matrix included three levels for most tuning parameters. 

Normally, a design that include three-level interactions will allow the production of quadratic models and identification of significant quadratic terms, and thus provide more precise predictions. 

However, the alternative design only recommends 22 runs instead of utilizing more runs. A full quadratic model would require 1+2*6+7*(7-1)/2 + 1 = 35 degree of freedom, i.e. 35 runs. Using 22 runs means there will be serious aliasing issue and we will not be able to estimate for most terms.

While the alternative design might allow 35-22 = 13 runs for confirmation experiments. We do not need as many as 13 runs for confirmation experiment

```{r}
# Create color map on pairwise correlations for 22-run design.
X.alt <- model.matrix(~.^2-1, data.frame(c_alter))
contrast.vectors.correlations.alt <- cor(X.alt)
corrplot(contrast.vectors.correlations.alt, type = "full", addgrid.col = "gray", tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.5)
```


## Question 5. Collect data using your recommended design in Question 3.

```{r}
# We convert the coded level to the actual level
I.design <- optimal_design_I$design

I.design$ntree[I.design$ntree == 1] <- 1000
I.design$ntree[I.design$ntree == -1] <- 100

I.design$mtry[I.design$mtry == 1] <- 6
I.design$mtry[I.design$mtry == -1] <- 2

I.design$replace[I.design$replace == -1] <- 0

I.design$nodesize[I.design$nodesize == 1] <- 11
I.design$nodesize[I.design$nodesize == -1] <- 1

I.design$classwt[I.design$classwt == 1] <- 0.9
I.design$classwt[I.design$classwt == -1] <- 0.5

I.design$cutoff[I.design$cutoff == 1] <- 0.8
I.design$cutoff[I.design$cutoff == -1] <- 0.2

I.design$maxnodes[I.design$maxnodes == 1] <- 1000
I.design$maxnodes[I.design$maxnodes == -1] <- 10

I.design
```

```{r}
load(file = "heart.RData")
source("CrossValidation_RF.R")
cv.rf(I.design, y, X)
```

```{r}
# Collect data
data_cv <- c(0.6434052, 0.7126352, 0.7133776, 0.6428887, 0.7112933,
             0.5000044, 0.4999999, 0.5000000, 0.5000045, 0.5000000,
             0.6635865, 0.7152352, 0.6589157, 0.5156135, 0.5546583,
             0.5534090, 0.5222387, 0.5653842, 0.6243382, 0.6645824,
             0.5175735, 0.5169417, 0.6432756, 0.5240403, 0.5012533,
             0.6774713, 0.6763149, 0.6961511, 0.6990668, 0.6944489,
             0.6749914, 0.7026534)
```

## Question 6. Conduct a detailed data analysis. What are the influential tuning parameters? What is the final model that links the tuning parameters to the cross-validation accuracy? Does the final model provide a good fit to the data?

```{r}
# Null model: the main effect and the interactions between two factors
new_data <- cbind(optimal_design_I$design, data_cv)
null_model <- lm(data_cv~.^2, data=new_data)
summary(null_model)
```

```{r}
# Simplified model: only include significant parameters
alter_model <- lm(data_cv ~ classwt+cutoff+classwt:maxnodes, data=new_data)
summary(alter_model)


# Check normality
par(mfrow=c(1,2))
res <- alter_model$residuals
qqnorm(res); qqline(res)
# Check constant variance
plot(alter_model$fitted.values, res, xlab = 'Predicted values', ylab = 'Residuals', main = "Residual vs Fitted")
abline(h = 0)
```

```{r}
# Recommened level
obj_func <- function(x){
  predicted <- 0.615910-0.060241*x[1]+0.026423*x[3]+0.025019*x[1]*x[2]
  -predicted
}

optim(par = c(0, 0, 0), fn = obj_func, lower = -1, upper = 1, method = "L-BFGS-B")
```

We would recommend setting â€˜classwtâ€˜ with low level, â€˜maxnodesâ€˜ with low level, and â€˜cutoffâ€˜ with high level to maximize the accuracy of cross-validation.

```{r message=FALSE, warning=FALSE}
# Confirmation Experiment
factors <- list(ntree = c(100, 1000),
                mtry = c(2, 6),
                replace = c(0, 1),
                nodesize = c(1,11),
                classwt = c(0.5, 0.9),
                cutoff = c(0.2, 0.8),
                maxnodes = c(10, 1000))
full_factorial_design <- FrF2(nruns = 2^7, 
                  nfactors = 7, 
                  randomize = F, 
                  factor.names = factors)

a <- full_factorial_design
b <- I.design

x <- sapply(a, is.factor)
a[ , x] <- as.data.frame(apply(a[ , x], 2, as.numeric))

x <- sapply(b, is.factor)
b[ , x] <- as.data.frame(apply(b[ , x], 2, as.numeric))

# the rest of 128-32=96 did not use designs
library(dplyr)
D <- setdiff(a,b)

# randomly factor settings for three designs
set.seed(123)
D <- D[sample(1:nrow(D), 3), ]  
D

load(file = "heart.RData")
source("CrossValidation_RF.R")
cv.rf(D, y, X)

# Actual response
confirm_cv <- c(0.7167379, 0.7016937, 0.6349859)

# Transform to the coded level
p1 <- data.frame(ntree = 1, mtry =1, replace = -1,
           nodesize = 1, classwt = -1, cutoff = 1,
           maxnodes = -1)
p2 <- data.frame(ntree = 1, mtry =1, replace = -1,
           nodesize = 1, classwt = -1, cutoff = 1,
           maxnodes = 1)
p3 <- data.frame(ntree = -1, mtry =-1, replace = 1,
           nodesize = -1, classwt = -1, cutoff = -1,
           maxnodes = 1)

v1 <- predict(alter_model, p1)
v2 <- predict(alter_model, p2)
v3 <- predict(alter_model, p3)
predict_v <- c(v1,v2,v3)
predict_v <- as.numeric(predict_v)

# Predictions
predict_v

# Judge the predictions
as.numeric(abs(confirm_cv - predict_v))
```

